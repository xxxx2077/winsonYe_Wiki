# 腾讯会议实习总结

## 概览

负责数据集成项目的主要调研，独立开发从MySQL到腾讯自研数仓TDW的离线数据同步程序，完成千万条历史数据的迁移和新数据的上报，解决了数据上报存在大量人工程序和相关信息未得到有效维护的问题，实现了高达99%的流程自动化和信息维护度

## FAQ

### 数据同步过程 & 实习解决了什么问题

**原先的方案：使用TDBank数据采集平台 + US任务调度平台**

1. TDBank人工填写数据源信息（包括MySQL库表名、字段名和字段类型信息）创建数据同步任务
2. US任务调度平台创建三个任务执行，分别为：数据库写入TDBank任务、检查对账文件任务，TDBank写入TDW任务

痛点：
1. 数据上报流程中存在大量人工程序，费时费力，容易出错（需要人工填写字段名和字段类型，存在命名规范的问题）
2. 数据上报相关信息未得到有效维护（数据库配置/源表/目标表/数据可用日期/任务链接）

要解决的问题：
1. 将数据上报流程自动化
2. 将数据上报相关信息维护起来

**现在的方案：使用Wedata大数据平台**

优点：

1. 仍然需要人工填写MySQL库表名，但是不需要人工填写「字段名和字段类型信息」（这部分信息Wedata自动获取）
2. 不需要人为跟踪任务信息（指在TDBank点击任务链接，跳转到US调度平台而且只能看见对应的某个任务，要看上下游任务还得在US平台人为点击链接查看），现在只需要在Wedata查看创建的一个任务即可。

亮点：
1. 新方案的制定本人全程参与甚至是主导
2. 旧数据迁移方案由本人和mentor共同思考得出，我想出了改变任务上下游的方式，mentor想到使用中间库的方式。

采用新方案分为两个部分：

- 旧数据迁移。原先使用TDBank + US的数据同步任务迁移到Wedata数据同步任务
- 新数据同步。直接创建Wedata数据同步任务。

具体看PPT

### 字段自动同步原理

开启自动变更后，每次任务执行时，Wedata能够检测字段变更，新增字段并调整字段映射关系

整个Wedata数据同步与数据接入比较类似，都是基于[DataX](https://github.com/alibaba/DataX)改编的，操作界面和[DataX](https://github.com/alibaba/DataX)比较类似。

不同之处在于：数据同步模块底层采用了监听binlog的数据采集方式，与[Canal](https://github.com/alibaba/canal)非常类似。而没有使用DataX

Wedata通过扫描mysql的schema，与之前的schema做差集，获取字段的变化


!!! info "SQL语句分类"

    - DDL，即数据定义语言（Data Definition Language），是SQL的一个子集，主要用于定义或改变数据库结构、数据表结构等。DDL语句不涉及表中的实际数据，而是对数据库对象（如表、视图、索引等）进行操作。执行DDL语句通常会立即生效，并且自动提交当前事务。
    - DML（Data Manipulation Language，数据操作语言）：
        - 用于对数据库中的数据进行操作，如查询、插入、更新和删除。
        - 常见命令包括：
            - SELECT：从数据库中检索数据。
            - INSERT：向表中添加新记录。
            - UPDATE：修改现有记录的数据。
            - DELETE：从表中删除记录。
    - DCL（Data Control Language，数据控制语言）：
        - 用于控制访问权限和安全级别。
        - 常见命令包括：
            - GRANT：授予用户或角色访问权限。
            - REVOKE：撤销用户或角色的访问权限。
    - TCL（Transaction Control Language，事务控制语言）：
        - 用于管理数据库中的事务，确保数据的一致性和完整性。
        - 常见命令包括：
            - COMMIT：提交当前事务，使事务对数据库所做的更改永久化。
            - ROLLBACK：撤销当前事务所做的所有更改，并恢复到事务开始前的状态。
            - SAVEPOINT：在事务中设置保存点，允许部分回滚。
            - SET TRANSACTION：设置事务的特性，比如隔离级别。
    - DQL（Data Query Language，数据查询语言）：
        - 实际上，SELECT语句有时也被单独归类为DQL，因为它专门用于查询数据。
        - 这是唯一的一种DQL命令，即SELECT，它用于从数据库中检索数据。
  
Wedata还有数据接入模块，这部分与[DataX](https://github.com/alibaba/DataX)比较类似

!!! tip

    本业务没有使用数据接入模块，因为本业务只聚焦于MySQL的离线数据同步，而不关心其他数据源，使用数据同步模块成本更小。

### 什么时候增量同步，什么时候全量同步



数据同步可以分为全量同步和增量同步：

- 全量同步：指将所有数据从源系统复制到目标系统，适用于首次数据加载或者数据完全不一致的情况。
- 增量同步：仅传输自上次同步以来发生变化的数据（如新增、修改或删除的数据），这种方式可以显著减少同步时间和资源消耗，适合于需要频繁更新数据的场景。

具体来说，增量同步就是**补充where条件筛选部分**，全量同步时，这部分为空。

[操作过程类似于这个视频](https://www.bilibili.com/video/BV1hfvKenEsS?t=382.8)

在本次业务中，一般情况下采用的是全量同步（因为我们已经进行了分库分表，每张表的数据量不会很大），如果数据量比较大采用增量同步。

## 背景

### 数据仓库

数据仓库，顾名思义：存储海量数据的仓库。数据仓库是一款将不同源头的数据（包括关系型数据库、文件系统、应用程序日志以及其他形式的数据源）整合起来，从而进行海量数据分析和报告的存储系统。

数据库和数据仓库不同之处在于：数据库通常存储特定业务领域的数据，而数据仓库存储整个企业的当前数据和历史数据，并为商业智能和分析提供数据支持。

典型的数据仓库包含 4 个核心组件：中央数据库、ETL（提取、转换、加载）工具、元数据和访问工具。

- 中央数据库：数据库是数据仓库的基础。传统上，这些数据库都是在本地或云端运行的标准关系数据库。但是，由于大数据的出现，再加上企业需要了解真实的实时绩效，以及 RAM 的成本大幅下降，内存数据库得到迅速普及。

- 数据集成：企业可以通过各种数据集成方法从源系统中提取数据并进行修改，从而提高一致性，助力快速分析。这些数据集成方法包括 ETL（提取、转换和加载）和 ELT、实时数据复制、批量加载处理、数据转换以及数据质量和丰富服务。

- 元数据：元数据是关于数据的数据，规定了数据仓库中数据集的来源、使用、价值和其他特性。业务元数据描述的是数据情境信息，技术元数据描述的是如何访问数据，包括数据的位置和结构。

- 数据仓库访问工具：借助这些访问工具，用户可以与数据仓库中的数据进行交互。访问工具包括查询和报告工具、应用开发工具、数据挖掘工具、联机分析处理 (OLAP) 工具等。

#### 数据仓库架构

- 数据层：通过 ETL 工具从数据源中提取数据，然后进行转换并加载到底层。底层包括数据库服务器、数据集市和数据湖。元数据就是在这一层创建，数据集成工具（比如数据虚拟化工具）用于对数据进行无缝整合和汇总。

- 语义层：在中间层，联机分析处理 (OLAP) 和联机事务处理 (OLTP) 服务器会重组数据，支持快速、复杂的查询和分析。

- 分析层：顶层是前端客户层。这一层部署了数据仓库访问工具，支持用户与数据交互、创建仪表盘和报表、监控 KPI、挖掘和分析数据、构建应用等。分析层通常包含工作台或沙箱区域，用于支持数据挖掘和新数据模型开发。

#### 数据湖

数据仓库和数据湖都能够存储大数据，但却是截然不同的存储系统。

数据仓库主要存储根据特定用途进行格式化的数据，而数据湖则存储未经处理的原始数据，其数据用途尚不明确。

数据仓库和数据湖通常相辅相成。例如，当需要使用数据湖中存储的原始数据来回答业务问题时，可以在数据仓库中对其进行提取、清理和转换后用于分析。数据量、数据库性能和存储价格是帮助企业选择合适存储解决方案的重要因素。

### 数据集成

数据集成是指将分布在不同位置、不同格式或不同平台的数据整合到一个统一的视图中，以便于访问、管理和分析的过程。它涉及到从**多个异构数据源**中提取数据，转换这些数据以符合目标系统的规范，并最终加载到目标系统中（通常称为ETL过程：Extract, Transform, Load）。

- 数据抽取（Extract）：从不同的源头收集数据。这些源头可以包括数据库（如MySQL、Oracle）、文件系统、Web服务接口等。这个阶段需要解决如何有效地连接各种数据源并获取所需数据的问题。
- 数据转换（Transform）：对抽取的数据进行清洗、转换操作，使其符合目标系统的格式要求。这可能包括数据类型的转换、去除重复记录、填充缺失值、数据标准化等步骤。
- 数据加载（Load）：将转换后的数据加载到目标系统中。目标系统可能是数据仓库、数据湖或其他类型的数据存储库。加载过程中需要考虑性能优化以及如何高效地处理大量数据。
- 元数据管理：管理关于数据本身的信息，比如数据来源、数据定义、数据质量规则等，这对于保证数据的一致性和可靠性至关重要。
- 数据质量和治理：确保数据的准确性、完整性和一致性，同时遵守相关的法律法规和公司政策。
- 实时与批量处理：根据业务需求选择合适的数据集成方式，既可以是定期执行的批量处理，也可以是几乎即时响应变化的流式处理。

数据集成的重要性

提升决策质量：通过整合来自不同部门或业务线的数据，管理层可以获得更全面的信息，从而做出更加明智的决策。

### Canel

[原理](https://www.cnblogs.com/bigdatalearnshare/p/13832709.html)

### （补充）数据接入模块

本数据同步方案与DataX非常相似，因此可以参考以下文章

[DataX数据同步](https://juejin.cn/post/7077744714954309669)

将不同数据源的同步抽象为从源头数据源读取数据的Reader插件，以及向目标端写入数据的Writer插件

因此我们要做的：

1. 指定数据源（源数据源和目标数据源）【这一步对应TDBank，由TDBank完成数据的采集，创建TDBank数据同步任务后】
2. 创建数据同步任务，我们称为Job。DataX Job 启动后，会根据不同源端的切分策略，将 Job 切分成多个小的 Task (子任务)，以便于并发执行。接着 DataX Job 会调用 Scheduler 模块，根据配置的并发数量，将拆分成的 Task 重新组合，组装成 TaskGroup（任务组）。每一个 Task 都由 TaskGroup 负责启动，Task 启动后，会固定启动 Reader --> Channel --> Writer 线程来完成任务同步工作。DataX 作业运行启动后，Job 会对 TaskGroup 进行监控操作，等待所有 TaskGroup 完成后，Job 便会成功退出（异常退出时 值非 0）【这一步对应TDBank和US任务调度平台，TDBank创建数据同步任务后，US上能看到创建的三个任务，由US管理这三个数据同步任务】
3. 由任务调度中心管理Job切分为多少个Task，并且根据用户配置并发数，计算分配多少个TaskGroup
4. 数据同步完成后更新至TDW

